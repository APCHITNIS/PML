---
title: "PML Project"
date: "December 18, 2016"
output: html_document
---

## Load the data.

```{r}
library(caret)
set.seed(12345)
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("pml-training.csv")){
        download.file(trainURL, "pml-training.csv")
}
if(!file.exists("pml-testing.csv")){
        download.file(testURL, "pml-testing.csv")
}

#Read the data sets
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testing <- read.csv("pml-testing.csv",na.strings = c("NA", "#DIV/0!", ""))
dim(training)
dim(testing)
```
The training data set consists of 19622 values of 160 variables.

## Clense the data sets.

### Remove Near Zero Variables. 

```{r}
nsv <- nearZeroVar(training)
training <- training[, -nsv]
testing <- testing[, -nsv]


```
### Next, remove the columns that from their description seem like they would not affect the prediciton outcome. The first five columns can be removed. 

```{r}
# remove columns that may not affect prediction
training <- training[, -(1:5)]
testing <- testing[, -(1:5)]
```

### Next, remove the columns with No variability. 

```{r}
# remove columns with no variability

remNoVariabilityCols <- sapply(training, function(x) mean(is.na(x))) > 0.95
training <- training[, remNoVariabilityCols == F]
testing <- testing[, remNoVariabilityCols == F]
dim (training)
dim(testing)
```

### After cleasing we reduced the training data set to 54 variables.

## Validation Set 

### Split the training data set into train and validation data sets. To save the computer resources I have created data partition with just 25% of data from the training data set. 

```{r}
# To save computer resources create data partition with just 25% of data in train model.
inTrain <- createDataPartition(training$classe, p=0.25, list = FALSE)
subTraining <- training[inTrain,]
subTesting <- training[-inTrain,]
dim(subTraining)
dim(subTesting)
```

## Train Model

### Next, We train the model using Random Forest with a cross validation of 4 folds to avoid overfitting.
```{r}
library(randomForest)
fitControl <- trainControl(method = "cv", number = 4)
modFit <- train(classe ~ ., data = subTraining, method = "rf", trControl = fitControl, prox = TRUE, allowParallel=TRUE)

```

## Cross Validation Testing
```{r}
predTrain <- predict(modFit, subTesting)
cm <- confusionMatrix(predTrain, subTesting$classe)
cm$table
```
We can notice that there are very few variables out of this model.

## Estimate of Sample Error


```{r}
accur <- postResample(subTesting$classe, predTrain)
modAccuracy <- accur[[1]]
modAccuracy
```
```{r}
out_of_sample_error <- 1 - modAccuracy
out_of_sample_error
```

### Both Accuracy of the model and the sample error are satisfactory. So our model is good to go!

## Applying the model to the test data set.

```{r}
answers <- predict(modFit, testing)
answers <- as.character(answers)

```

## Answers to the quiz
```{r}
answers
```

